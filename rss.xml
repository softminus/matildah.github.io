<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"
    xmlns:dc="http://purl.org/dc/elements/1.1/">
    <channel>
        <title>Kia's blog</title>
        <link>https://matildah.github.io</link>
        <description><![CDATA[Kia's blog]]></description>
        <atom:link href="https://matildah.github.io/rss.xml" rel="self"
                   type="application/rss+xml" />
        <lastBuildDate>Fri, 15 Jan 2016 00:00:00 UT</lastBuildDate>
        <item>
    <title>Leap seconds (considered harmful)</title>
    <link>https://matildah.github.io/posts/2016-01-15-leapseconds.html</link>
    <description><![CDATA[<div class="info">
    Posted on January 15, 2016
    
</div>

<p>If you have a good enough idea of what leap seconds, UTC, and TAI are, feel free to skip down to the “a damaging compromise” subheading below.</p>
<h1 id="whats-a-timescale">what’s a timescale?</h1>
<p>Measuring and synchronizing and transferring time needs a concept of a <em>timescale</em> – a standard that both defines the <strong>rate of advance</strong> of time and also one (or more) <strong>reference points</strong>. For example, I could define a timescale <em>t</em> (with arbitrary units) that has t=0 at the point I published this post, and that increments by 1 unit every time a pendulum I am holding completes a full cycle. Note that multiple timescales can coexist, just as multiple units of temperature can coexist – if you know the zero points and the rates of advance, you can translate from one to another rather easily. Usually, zero points can be chosen arbitrarily (such as the famous <a href="https://en.wikipedia.org/wiki/Unix_time#Encoding_time_as_a_number">Unix Epoch</a>), because as long as two timescales tick at the same rate, converting between them if they have different epochs / zero points is trivial for computers and doesn’t add confusion or ambiguity (unless you want to deal with times before the epoch, which might involve negative numbers).</p>
<h1 id="atoms-for-time">atoms for time:</h1>
<p>The rate of advance of timescales is a trickier thing. If you’re going around defining timescales, you want that to be dependent on an easily and accurately reproducible physical phenomenon, so you can set up your own standard and be able to keep/transfer time on your own. Nowadays we have atomic frequency standards, which work by using the properties of a sample of atoms (usually cesium or rubidium) to generate an extremely stable frequency that (with careful equipment design) doesn’t depend on any external factors. The atoms have a bunch of energy levels and can absorb/release energy (in form of electromagnetic radiation) to change energy levels – but here we’re only interested in one specific transition: for cesium it’s a specific transition that releases/absorbs radiation of a frequency of 9,192,631,770 Hz. This number is only a function of the cesium atom – as long as the experiment setup and equipment don’t introduce inaccuracies, anyone can induce that transition (by exciting the cesium atoms with the right frequency of microwaves) and everyone will measure the same frequency.</p>
<p>What the atomic frequency standard does is conceptually simple – it adjusts the frequency of the oscillator that is coupled into the sample cell and (with methods that aren’t really relevant here) measures how many of the atoms inside the sample cell undergo the specific energy level transition we want to induce. If our electronic oscillator is “out of tune” with our sample cell atoms, very few will undergo transition – however, if we’re making microwaves at the same frequency the atoms accept, many will undergo transition. The electronics implement a feedback loop that works to maximise the number of atoms that undergo transition. If everything works right, this physics experiment has done something quite remarkable – if we tap off some of the variable frequency oscillator’s output, we have a frequency generator that generates a frequency not determined by anything specific to our experiment or any of the electronic components. As long as our microwave oscillator is in tune with the atoms in the sample cell, the frequency it generates depends only on the fundamental properties of cesium atoms. Cesium atoms don’t fall apart or go out of tune or undergo frequency drift, and if you purify cesium and I purify cesium we’ll get atoms that are identical. The convenience of cesium atoms for doing this sort of thing has led to the second being <strong>defined</strong> in terms of cesium atoms – you build that cesium frequency standard and count off 9,192,631,770 cycles on that oscillator that’s in tune with the cesium atoms, and how long that takes <strong>is defined to be one SI second</strong>. The cesium atoms will always tick at 9,192,631,770 pulses per second, but you can use electronics that count the pulses generated by the cesium standard and only output a pulse when 9,192,631,770 cesium pulses have gone by – that lets you have a pulse-per-second output that is derived from the cesium standard. You can use this <a href="https://en.wikipedia.org/wiki/Frequency_divider">frequency division</a> method to generate any frequency of pulse you want from the raw, 9.192 GHz cesium standard output.</p>
<h1 id="earth-time-vs-atom-time">Earth time vs atom time:</h1>
<p>Cesium frequency standards are wonderful and good and keep excellent time and everyone uses them and the GPS system depends on them and all timekeeping that involves computers and networks depends on them – either by computers attached to atomic clocks, or by computers attached to GPS receivers that get time that depends on atomic clocks in the GPS systems. There’s one issue here: <em>before we had cesium clocks, we already had defined the second</em>. This isn’t inherently trouble – we could just have defined the “atomic” second so that whatever old, non-atomic, definition of the second would match up, and just go on with life, except with better time/frequency-keeping. However, there’s a massive problem – the previous definition of the second was based on the <strong>rotation angle of the Earth</strong> (that thing that generates days and nights rather periodically) and <strong>the Earth’s rotation is slowing down</strong>. Oh no :(. They’re fundamentally different timescales, and it has nothing to do with their ticking rates being different – it’s because atomic frequency standards have a tick rate that <strong>doesn’t change as they get older</strong> and the Earth has a mean solar day that gets a tiny bit longer as it gets older.</p>
<h1 id="a-damaging-compromise">A damaging compromise:</h1>
<p>There’s an awkward problem here, and the standards bodies behind timekeeping have dealt with it in an unclean and damaging way. The current state of affairs is this: just about every time you see or work with is on a timescale called UTC (or converted to local time with a constant hour:minute offset from UTC). The UTC timescale is a botched up hybrid of two timescales: <a href="https://en.wikipedia.org/wiki/UT1#Versions">UT1 (Universal Time 1)</a>, derived entirely from (smoothed-out) Earth rotation, and <a href="https://en.wikipedia.org/wiki/International_Atomic_Time">TAI (International Atomic Time)</a>, derived entirely from atomic clocks. UTC (Universal Coordinated Time) normally ticks off each second in sync with TAI, which makes it seem deceptively easy to handle. However, UTC is defined such that when the <a href="https://en.wikipedia.org/wiki/DUT1">offset between UTC and UT1</a> gets too large, a <a href="https://en.wikipedia.org/wiki/International_Earth_Rotation_and_Reference_Systems_Service">standards bureau</a> decides that UTC needs to <a href="https://en.wikipedia.org/wiki/Leap_second#Insertion_of_leap_seconds">take a time-out</a> (known as a “leap second”) and let UT1 (the Earth’s rotation) catch up. During the leap second, UTC has a mildly nonsensical value – rather than going from 23:59:59 to 00:00:00 as usual, UTC on the leap second-affected day goes from 23:59:59 to 23:59:60 to 00:00:00. While this fulfills the goals of UTC – use constant second lengths that match up with the cesium-derived SI standard second but regularly insert leap seconds to keep UTC in sync with the Earth’s rotation – <a href="https://en.wikipedia.org/wiki/Leap_second#Examples_of_problems_associated_with_the_leap_second">leap seconds cause significant problems</a> for computers, networks, navigational systems, financial systems, air traffic control systems, and allegedly, even <a href="http://queue.acm.org/detail.cfm?id=1967009">US nuclear weapons systems</a>. Note that leap seconds aren’t even deterministic – they depend on measurements of the Earth’s rotation. Software not only chokes on the leap second, but can’t even know more than a few months in advance if there’ll even <em>be</em> a leap second.</p>
<p>The problem with leap seconds not due to shoddy coding. There are irreconcilable differences between the semantics of time that most software depend on and the semantics of UTC around leap seconds. Aside from conveniently scheduling maintenance/downtime periods to coincide with leap second days, there are two fundamental schools of thought about how to deal with leap seconds – the first is to make all software (all application software, libraries, operating systems, NTP software) aware of leap seconds and correctly operate around them. This is a nontrivial task – every calculation of times and dates and time intervals is affected, as is storage and transmission of time/date values. This is a deeply fragile and devastatingly difficult solution – if you forget to take into account leap seconds once, everything can get messed up in your software. This is not an approach to dealing with complexity that has a good record in software engineering (ask a C programmer about memory safety).</p>
<h1 id="bad-standards-utc-and-a-proven-alternative-tai-the-timezone-database">Bad standards (UTC) and a proven alternative (TAI + the timezone database)</h1>
<p>Using UTC with leap seconds for computers does not seem to be a good idea. The traditional response might be to blame programmers for failing to be sufficiently careful and detail-oriented but it’s not a useful or correct response. A standard that gives weird and non-deterministic semantics to the thing we expect to have the simplest and <strong>most</strong> deterministic semantics (outside of relativistic effects) – time itself – is a bad standard. djb’s <a href="https://cr.yp.to/talks/2015.01.07/slides-djb-20150107-a4.pdf">view</a> on cryptographic standards is applicable: if a standard leaves easily avoidable ways for the implementers of it to mess up, <strong>it’s a bad standard</strong>. Instead of having to redesign every bit of every system that manipulates/stores/transmits/computes with time to account for a nondeterministic leap second (that almost certainly doesn’t even have a good representation in whatever time formats are in use), we would like to get rid of UTC and use something like TAI (or anything that has a constant offset from TAI) to get rid of all leap second issues in one fell swoop. We can’t completely relegate UTC and leap seconds to the dustbin of history, because there <strong>are</strong> applications that need to use time that lines up nicely with days and nights: civil time, for example, will remain defined by UTC. This isn’t a problem though – as we already have a time-tested mechanism to convert time from a universal standard to a bunch of separate time standards that accounts for non-deterministic updates in time conversion rules that leap seconds would require: it’s called the <a href="https://en.wikipedia.org/wiki/Tz_database">timezone database</a>. Rather than injecting leap seconds directly into an otherwise unsullied atomic timescale, using TAI for time transfer (NTP) and internal software use and only adding leap seconds when needed with the timezone mechanism is a much cleaner separation of concerns. This is no untested proposal: this is what the GPS system (which depends on precise timing) uses. GPS time is TAI (with an offset of 19 seconds – but the offset never changes, so it’s irrelevant) and GPS navigational messages contains information to relate GPS time to UTC (how many leap seconds have been added to UTC so far).</p>
]]></description>
    <pubDate>Fri, 15 Jan 2016 00:00:00 UT</pubDate>
    <guid>https://matildah.github.io/posts/2016-01-15-leapseconds.html</guid>
    <dc:creator>Kia</dc:creator>
</item>
<item>
    <title>Introduction to MirageOS and NTP</title>
    <link>https://matildah.github.io/posts/2015-12-20-introduction.html</link>
    <description><![CDATA[<div class="info">
    Posted on December 20, 2015
    
</div>

<p>I’ve been getting started on my <a href="https://wiki.gnome.org/Outreachy/2015/DecemberMarch">Outreachy internship</a>, working with the <a href="https://mirage.io/">MirageOS</a> unikernel, (part of Xen). A unikernel is a modern way to organize application code along with systems code. Rather than having an application (along with the runtime system of whatever language it is written in) running on a fully general operating system in a hypervisor, unikernels work to allow collapse of non-necessary abstraction layers. Rather than a fully general OS that can run any application and handle its system calls, we have a set of libraries that provide similar services that an OS would provide with syscalls, along with a runtime system (for whatever language the unikernel is written in), and configuration tools that allow for each application to be compiled with the OS libraries and the runtime into a single bespoke image that can be loaded into the hypervisor and run.</p>
<p>There are several significant advantages to unikernels. All the code is executed in the same address space and context; there are no syscalls involved and no application/kernel privilege level changes (there are of course hypercalls and unikernel/hypervisor context switches but those are inevitable here). This makes analysis of the code much easier – everything besides the hypervisor itself is code that the compiler can see at one time and can optimize / typecheck appropriately. Avoiding userspace/kernel context switches is also kinder on the caches/TLBs – syscalls to the OS are replaced with function calls in the same address space! Also, while there’s no fundamental requirement for this, it’s possible to write almost all the code for a unikernel’s OS libraries in a strongly-typed functional programming language such as Haskell or OCaml as opposed to writing it in C.</p>
<p>There are a handful of servers for common network services/protocols already implemented in OCaml and that can run on MirageOS such as an <a href="https://github.com/mirage/mirage-www">http server</a> (that the MirageOS website is hosted on, a <a href="https://github.com/mirage/ocaml-dns">DNS server</a> and a <a href="https://mirage.io/blog/introducing-charrua-dhcp">DHCP server</a>, however a conspicuously missing one is NTP – the <a href="https://en.wikipedia.org/wiki/Network_Time_Protocol">Network Time Protocol</a>.</p>
<p>NTP allows a computer with a network connection to get its internal clock synchronized over the internet to a reference clock that has specialized timing hardware such as a GPS receiver or even a <a href="https://en.wikipedia.org/wiki/Atomic_clock">cesium or rubidium frequency standard</a>. An NTP server doesn’t do much – all it does is look at its time reference (either specialized timekeeping hardware or just another NTP server that itself has specialized time hardware) and answer client requests with the current time. An NTP client has to do two primary things with the information it gets from replies from an NTP server – the first is correcting the time offset between its clock so the time is, well, accurate. This isn’t enough, alas, as the timekeeping crystal oscillator in most computers isn’t too good (it doesn’t need to be, because of NTP &gt;_&lt;) – not only is its frequency inaccurate (it might run too fast, like a cheap digital watch that gains a few seconds per day), but the inaccuracy in frequency itself <strong>drifts</strong> as a function of temperature (and other factors).</p>
<p>An NTP client that wishes to try its best at keeping accurate time (as opposed to the kind that just periodically set the local clock to the time received by a reference server and completely <em>ignores</em> what happens to the local clock) needs a feedback loop that adjusts the phase (to minimize the time offset) and frequency (to minimize the frequency error) of the local clock, fed with measurements made (over the network) of reference servers. After some time, the feedback loop will converge and accurately track the local clock’s frequency offset and thus will keep accurate time without having to step the clock.</p>
<p>For this project, I’m working on implementing an NTP server and an NTP client in OCaml for the MirageOS unikernel. My plan is to make a purely functional NTP library that handles parsing/generating NTP packets and the state machine / filtering / clock discipline algorithms that is free of dependencies to UNIX. Once I am done with that, I will try to interface it to MirageOS’s clock interface (and modify it if necessary).</p>
]]></description>
    <pubDate>Sun, 20 Dec 2015 00:00:00 UT</pubDate>
    <guid>https://matildah.github.io/posts/2015-12-20-introduction.html</guid>
    <dc:creator>Kia</dc:creator>
</item>

    </channel>
</rss>
