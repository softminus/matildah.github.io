<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"
    xmlns:dc="http://purl.org/dc/elements/1.1/">
    <channel>
        <title>Kia's blog</title>
        <link>https://matildah.github.io</link>
        <description><![CDATA[Kia's blog]]></description>
        <atom:link href="https://matildah.github.io/rss.xml" rel="self"
                   type="application/rss+xml" />
        <lastBuildDate>Sat, 30 Jan 2016 00:00:00 UT</lastBuildDate>
        <item>
    <title>Single address spaces: design flaw or feature?</title>
    <link>https://matildah.github.io/posts/2016-01-30-unikernel-security.html</link>
    <description><![CDATA[<div class="info">
    Posted on January 30, 2016
    
</div>

<p>Unikernels operate in a single address space. Usually this is an address space provided by a hypervisor (or a microkernel) but there’s no reason you can’t run a single unikernel on a single CPU (“bare metal”) with no hypervisor involved. As unikernels become more well-known, I’ve seen people describe the single-address-space design choice in quite pejorative terms. Unfortunately, many explanations of unikernels do not adequately explain this decision to ditch the MMU. In the first part of this blog post, I will explain the performance advantages of living in a single address space and how even some high performance non-unikernel systems are designed to exploit those benefits. In the second, I will explore the security/correctness aspects of the unikernel address space model.</p>
<p>Most operating systems use MMU<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>-enforced isolation to both separate processes from each other and separate user code from kernel code. MMU-enforced isolation is a powerful tool – indeed, if you know how your MMU hardware will interpret its translation tables and know how your kernel writes to the translation tables, you can generate <a href="https://sel4.systems/Info/FAQ/proof.pml">formal results</a> about the isolation between processes or between kernel code and user processes. Outside of the kernel explicitly enabling processes to interact with each other (inter-process communication), the isolation properties that flow from a properly configured MMU are <em>independent of the user-level code being run</em>.</p>
<p>Properly set up MMU-enforced isolation provides isolation between malicious and unknown bits of code – but at a cost. Context switches take time:</p>
<ul>
<li>the MMU’s tables might have to be changed</li>
<li>some of the MMU’s TLB (a specialized <a href="https://en.wikipedia.org/wiki/Translation_lookaside_buffer">cache for translation results</a>) has to be invalidated</li>
<li>the CPU’s instruction and data caches need to be appropriately maintained (which might involve invalidation/flushing)</li>
<li>register state needs to be saved and restored</li>
<li>pipelines probably need to be flushed / reloaded</li>
</ul>
<p>The performance of context switches is crucial as every <a href="https://en.wikipedia.org/wiki/System_call">system call</a> a userspace program does to interact with hardware or OS services causes at least two context switches – one userspace-to-kernel, one kernel-to-userspace.</p>
<h3 id="context-switches-hurt-your-caches">Context switches <em>hurt</em> your caches…</h3>
<p>While there are hardware mechanisms <a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> to optimize context switches, they do not eliminate all the performance degradation. Context switches have indirect costs outside of the time used for the switch itself – all the previously-mentioned caches get utterly trashed. The code that is being <strong>switched to</strong> has to start off execution with a cold cache, deeply degrading performance. The degradation is mutual – as when control returns to the code that was originally running, it can take <strong>tens of thousands of cycles</strong> <a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a> for performance to return to baseline levels. Executing a system call like <code>write()</code> evicts <a href="https://www.usenix.org/legacy/events/osdi10/tech/slides/soares.pdf">2/3rds of the L1 cache and TLB</a>, deeply degrading user code performance depending on how many instructions happen between syscalls. There are hardware features that can reduce cache pollution. TLB lockdown mechanisms, for example, allow an OS to make a certain number of entries in the TLB permanent (so they aren’t overwritten by invalidations or replacement). For example, a kernel can lock down entries for its own address space to reduce translation table walks after a user-kernel switch – the TLB entries are already there! However, these architectural features cannot come close to making context switches painless in high-performance applications and can even have costs of their own. Locked down some TLB entries to make context switching hurt less? Unless your TLB has entries that can <em>only</em> be used for lockdown entries, the locked down entries will <strong>always take up space in the TLB</strong> that might be more usefully used by cached translation results needed by program workload.</p>
<p>Context switches are inherently<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> cruel to caches, and CPUs need happy caches in order to get acceptable performance. To reduce the performance impact of syscalls without modifying application software, <a href="https://www.usenix.org/conference/osdi10/flexsc-flexible-system-call-scheduling-exception-less-system-calls">exceptionless/asynchronous syscalls</a> have been demonstrated. With the regular syscall interface, the userspace process requests a syscall by executing a special software interrupt instruction to cause a context switch to the kernel. The arguments for the syscall are put in the general-purpose registers. The exceptionless syscall model requires small modifications to the libc and the kernel: when a syscall is requested from the application program, the libc places the syscall’s arguments in a special page of memory (“syscall page”) and switches to another user-level thread. The kernel, at its leisure, can look at each process’s syscall page, execute the syscalls, and return the results and set a completion flag in the syscall page. This leads to fewer context switches which amortizes the direct time cost of context switches. Both kernel and user code run longer uninterrupted, which means less time with the caches cold. This decoupling of control flow means that they can even assign one core to the application software and one to servicing its syscalls! The first core will stay in user mode and the second will stay in kernel mode; instead of user code and kernel code interrupting each other, they communicate through syscall pages<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a>. That the <a href="https://www.usenix.org/legacy/events/osdi10/tech/full_papers/Soares.pdf">cited paper</a> shows <strong>doubled<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a> performance</strong> in common workloads such as serving web pages <strong>just by replacing the syscall mechanism</strong> with an asynchronous one is a damning indictment.</p>
<h3 id="and-getting-rid-of-them-is-serious-business">…and getting rid of them is serious business</h3>
<p>Work done on software to reduce the performance degradation caused by syscalls and context switching is far from being a systems research curiosity. If you’re working with gigabits per second of small packets, you’re going to be dealing with lots of packets per second. If the regular kernel networking APIs require a syscall per packet, this forces a context switch and a copy of data from kernel space to user space (or from user space to kernel space) <strong>for every packet</strong>. At enough packets per second, literally all your cores are doing is switching context, copying data, and running all the time with polluted caches and TLBs, leading to severe performance degradation. You will <a href="https://blog.cloudflare.com/kernel-bypass/">not be able</a> to receive 10Gb of small packets with the regular Linux APIs.</p>
<p><a href="http://blog.tsunanet.net/2010/11/how-long-does-it-take-to-make-context.html">This microbenchmark</a> of context switches gives latency numbers on the order of 1 microsecond per context switch (which does <em>not</em> account for the slowdown due to polluted caches). 1 microsecond per context switch means <em>only one million context switches per second</em>, which isn’t close to the 10 million packets-per-second needed to saturate a 10Gb NIC. Because of this, your latency numbers will be <a href="https://blog.cloudflare.com/how-to-achieve-low-latency/"><em>ten times</em> higher</a><a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a> than what the hardware itself is capable of. To avoid the overhead of kernel-user data copying and the ruinous effect of context switches there are a bunch of different kernel APIs that just expose the NIC’s control and ring buffers to userspace. It’s your program’s responsibility to implement an TCP/IP stack in userspace with the send/receive ring buffers that are exposed to it. If the kernel is involved at all, all it has to do is verify the validity of addresses that userspace gave it and poke the NIC (if that can’t be done from userspace with appropriate configuration of mappings). Cloudflare (which is in the business of dealing with lots of gigabits of possibly-unwanted small packets) <a href="https://blog.cloudflare.com/single-rx-queue-kernel-bypass-with-netmap/">depends on</a> userspace / kernel-bypass networking. High-performance networking on general-purpose CPUs<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a> today, regardless of what’s being done, does <a href="https://github.com/SnabbCo/snabbswitch">not</a> go <a href="https://github.com/robertdavidgraham/masscan">anywhere</a> without some <a href="http://dpdk.org/">flavor</a> of <a href="https://software.intel.com/en-us/blogs/2015/06/12/user-space-networking-fuels-nfv-performance">kernel-bypass networking</a>. To reduce latency even further, Intel has <a href="http://www.intel.com/content/dam/www/public/us/en/documents/technology-briefs/data-direct-i-o-technology-brief.pdf">DDIO</a> that lets a network card <em>directly</em> place incoming packets in the Last Level Cache of a CPU without <em>any</em> access to main memory (and do the opposite upon transmit); a feature that only works to its full extent when the cache isn’t constantly being trashed by user/kernel context switching.</p>
<h3 id="back-to-unikernels">Back to unikernels</h3>
<p>What does this all have to do with unikernels? Well, an application that uses kernel-bypass networking (and that doesn’t depend on other kernel features) on Linux contains an ad hoc, informally-specified, bug-ridden, slow implementation of half of a unikernel<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a> – and using the Linux kernel as a hypervisor. If you don’t need kernel <em>services</em> (or can use them with a library you link against) and just need <em>isolation</em>, an actual hypervisor might be a better tool. If you need kernel features that provide a <strong>coherent and unified view of some hardware</strong> (especially one that contains lots of state, like a GPU or a filesystem on a disk) <strong>to multiple agents at once</strong> then the abstractions that a traditional kernel<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a> provides look more appealing.</p>
<p>However, if all you need is some address space of your own, access to a network device that’s just for you (nobody can mess with your NIC ring buffers!), and range of disk blocks for yourself, a unikernel’s libraries and language runtime can provide abstractions like a filesystem or threading or a a TCP/IP stack over those. Rather than the kernel and the MMU providing isolation between your code and the kernel, it’s the language’s <em>type system</em> and <em>memory safety</em> that provides isolation between your code and the unikernel’s libraries that provide the services your code uses.</p>
<p>The guarantees that a properly-managed MMU provide are just isolation between your process’s address space and the kernel’s address space: if your kernel or application is written in C and has an exploitable memory safety issue, the MMU won’t magically prevent exploitation. The type safety and memory safety of whatever code you end up running is always an issue and the best the MMU can do is limit the extent of the compromise. Indeed, unless the kernel (or hypervisor) is being exploited, a process under a traditional kernel running attacker-controlled code is as limited as a compromised unikernel in a hypervisor – except that kernels like Linux have a lot worse security record than hypervisors like Xen. Furthermore, unikernels can be written in memory-safe and type-safe languages and it is easier to reason about a OCaml program that links to OCaml libraries rather than software that calls upon a huge mass of kernel C code that has absolutely no documentation in most cases (much less a formal description of what it does). Instead of a kernel that needs to provide isolation and services to whatever arbitrary machine code is thrown at it, the runtime of the unikernel’s language only has to run a single language’s code that has been typechecked – a much easier task.</p>
<p>Using a unikernels doesn’t just have security advantages from using a single memory-safe and type-safe language – there are potential benefits in terms of debugging. Admittedly, production-quality introspection/instrumentation tooling isn’t currently available for unikernels, and this is something that needs to change. However, it’s easier to create quality tooling for something written in a single language with a decent type system that lives in a single address space than tools that having to create bespoke instrumentation code that lives in the kernel (and userspace tools to interface with it) for each component, each with its own data format and structures, in this veritable <a href="http://www.brendangregg.com/Perf/linux_observability_tools.png">menagerie</a> of kernel components.</p>
<p>Unikernels are appealing not just because they let us use a decent language with a good type system and memory safety instead of piles of C – they let us redraw isolation and abstraction boundaries with more appropriate and specific tools provided by our language’s type systems. When the intended application doesn’t involve the kernel running userspace code that is unknown or potentially hostile or not yours, a unikernel model where all the code lives in the same address space, is compiled and typechecked as a single compilation unit, and is managed by a single language runtime offers quite a few advantages.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>The <a href="https://en.wikipedia.org/wiki/Memory_management_unit#Overview">MMU</a> is a piece of hardware that can be configured by software running at an appropriate privilege level. Once the MMU is configured with a set of translation tables, all code running at that privilege level <strong>or lower</strong> will have its access to memory restricted by the parameters of the MMU. When your code generates an access to memory, the MMU takes in that memory address and looks at its translation tables and generates a result or an error. If the memory address is in a region that is not granted access by the translation tables, an CPU interrupt is generated. If the MMU searches its tables and finds an entry that includes the input memory address, it will use the information in that page table entry to generate a <em>translated address</em>. If there is no other MMU in the system, that address is used to directly access memory (and is termed a “physical address”). A CPU that supports virtualization will have two MMUs and thus two levels of translation. There is a Stage 1 MMU that is configured by the guest OS and converts addresses from userspace software (virtual addresses) to intermediate physical addresses (if there weren’t a hypervisor they’d be real physical addresses!). The Stage 2 MMU, configured by the hypervisor, takes in intermediate physical addresses and converts them into physical addresses, fully fit to be used to access memory.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>For example, the hardware can tag each TLB entry with an address space identifier number and have a register (that the OS changes upon context switch) for the current address space identifier and only use TLB entries that have a matching tag.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p><a href="https://www.usenix.org/legacy/events/osdi10/tech/full_papers/Soares.pdf">“There is a significant drop in instructions per cycle (IPC) due to the system call, and it takes up to 14,000 cycles of execution before the IPC of this application returns to its previous level. As we will show, this performance degradation is mainly due to interference caused by the kernel on key processor structures”</a><a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Caches like temporal and spatial locality. Synchronous system calls break those.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>The syscall pages tend to stay cached meaning that the cache coherency mechanism (and not slow main memory accesses) is used to transmit them from userspace cores to kernel cores<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p><a href="https://www.usenix.org/legacy/events/osdi10/tech/full_papers/Soares.pdf">“We show how FlexSC improves performance of Apache by up to 116%, MySQL by up to 40%, and BIND by up to 105% while requiring no modifications to the applications.”</a><a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>And right around what we’d expect to see, having seen the numbers for context switch timing.<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>And not FPGAs or ASICs.<a href="#fnref8">↩</a></p></li>
<li id="fn9"><p>Instead of having the kernel expose the NIC’s ring buffers to you, the hypervisor does it. Instead of a filesystem you do syscalls to fuss with, the hypervisor gives you a range of disk blocks and you can call functions in some unikernel library that implement a filesystem.<a href="#fnref9">↩</a></p></li>
<li id="fn10"><p>It doesn’t have to be implemented <em>at all like</em> a traditional monolithic kernel – you could have a unikernel that provides a coherent/unified filesystem service to multiple other unikernels and use the hypervisor just like you’d use a microkernel to pass messages between VMs.<a href="#fnref10">↩</a></p></li>
</ol>
</div>
]]></description>
    <pubDate>Sat, 30 Jan 2016 00:00:00 UT</pubDate>
    <guid>https://matildah.github.io/posts/2016-01-30-unikernel-security.html</guid>
    <dc:creator>Kia</dc:creator>
</item>
<item>
    <title>Leap seconds (considered harmful)</title>
    <link>https://matildah.github.io/posts/2016-01-15-leapseconds.html</link>
    <description><![CDATA[<div class="info">
    Posted on January 15, 2016
    
</div>

<p>If you have a good enough idea of what leap seconds, UTC, and TAI are, feel free to skip down to the “a damaging compromise” subheading below.</p>
<h1 id="whats-a-timescale">what’s a timescale?</h1>
<p>Measuring and synchronizing and transferring time needs a concept of a <em>timescale</em> – a standard that both defines the <strong>rate of advance</strong> of time and also one (or more) <strong>reference points</strong>. For example, I could define a timescale <em>t</em> (with arbitrary units) that has t=0 at the point I published this post, and that increments by 1 unit every time a pendulum I am holding completes a full cycle. Note that multiple timescales can coexist, just as multiple units of temperature can coexist – if you know the zero points and the rates of advance, you can translate from one to another rather easily. Usually, zero points can be chosen arbitrarily (such as the famous <a href="https://en.wikipedia.org/wiki/Unix_time#Encoding_time_as_a_number">Unix Epoch</a>), because as long as two timescales tick at the same rate, converting between them if they have different epochs / zero points is trivial for computers and doesn’t add confusion or ambiguity (unless you want to deal with times before the epoch, which might involve negative numbers).</p>
<h1 id="atoms-for-time">atoms for time:</h1>
<p>The rate of advance of timescales is a trickier thing. If you’re going around defining timescales, you want that to be dependent on an easily and accurately reproducible physical phenomenon, so you can set up your own standard and be able to keep/transfer time on your own. Nowadays we have atomic frequency standards, which work by using the properties of a sample of atoms (usually cesium or rubidium) to generate an extremely stable frequency that (with careful equipment design) doesn’t depend on any external factors. The atoms have a bunch of energy levels and can absorb/release energy (in form of electromagnetic radiation) to change energy levels – but here we’re only interested in one specific transition: for cesium it’s a specific transition that releases/absorbs radiation of a frequency of 9,192,631,770 Hz. This number is only a function of the cesium atom – as long as the experiment setup and equipment don’t introduce inaccuracies, anyone can induce that transition (by exciting the cesium atoms with the right frequency of microwaves) and everyone will measure the same frequency.</p>
<p>What the atomic frequency standard does is conceptually simple – it adjusts the frequency of the oscillator that is coupled into the sample cell and (with methods that aren’t really relevant here) measures how many of the atoms inside the sample cell undergo the specific energy level transition we want to induce. If our electronic oscillator is “out of tune” with our sample cell atoms, very few will undergo transition – however, if we’re making microwaves at the same frequency the atoms accept, many will undergo transition. The electronics implement a feedback loop that works to maximise the number of atoms that undergo transition. If everything works right, this physics experiment has done something quite remarkable – if we tap off some of the variable frequency oscillator’s output, we have a frequency generator that generates a frequency not determined by anything specific to our experiment or any of the electronic components. As long as our microwave oscillator is in tune with the atoms in the sample cell, the frequency it generates depends only on the fundamental properties of cesium atoms. Cesium atoms don’t fall apart or go out of tune or undergo frequency drift, and if you purify cesium and I purify cesium we’ll get atoms that are identical. The convenience of cesium atoms for doing this sort of thing has led to the second being <strong>defined</strong> in terms of cesium atoms – you build that cesium frequency standard and count off 9,192,631,770 cycles on that oscillator that’s in tune with the cesium atoms, and how long that takes <strong>is defined to be one SI second</strong>. The cesium atoms will always tick at 9,192,631,770 pulses per second, but you can use electronics that count the pulses generated by the cesium standard and only output a pulse when 9,192,631,770 cesium pulses have gone by – that lets you have a pulse-per-second output that is derived from the cesium standard. You can use this <a href="https://en.wikipedia.org/wiki/Frequency_divider">frequency division</a> method to generate any frequency of pulse you want from the raw, 9.192 GHz cesium standard output.</p>
<h1 id="earth-time-vs-atom-time">Earth time vs atom time:</h1>
<p>Cesium frequency standards are wonderful and good and keep excellent time and everyone uses them and the GPS system depends on them and all timekeeping that involves computers and networks depends on them – either by computers attached to atomic clocks, or by computers attached to GPS receivers that get time that depends on atomic clocks in the GPS systems. There’s one issue here: <em>before we had cesium clocks, we already had defined the second</em>. This isn’t inherently trouble – we could just have defined the “atomic” second so that whatever old, non-atomic, definition of the second would match up, and just go on with life, except with better time/frequency-keeping. However, there’s a massive problem – the previous definition of the second was based on the <strong>rotation angle of the Earth</strong> (that thing that generates days and nights rather periodically) and <strong>the Earth’s rotation is slowing down</strong>. Oh no :(. They’re fundamentally different timescales, and it has nothing to do with their ticking rates being different – it’s because atomic frequency standards have a tick rate that <strong>doesn’t change as they get older</strong> and the Earth has a mean solar day that gets a tiny bit longer as it gets older.</p>
<h1 id="a-damaging-compromise">A damaging compromise:</h1>
<p>There’s an awkward problem here, and the standards bodies behind timekeeping have dealt with it in an unclean and damaging way. The current state of affairs is this: just about every time you see or work with is on a timescale called UTC (or converted to local time with a constant hour:minute offset from UTC). The UTC timescale is a botched up hybrid of two timescales: <a href="https://en.wikipedia.org/wiki/UT1#Versions">UT1 (Universal Time 1)</a>, derived entirely from (smoothed-out) Earth rotation, and <a href="https://en.wikipedia.org/wiki/International_Atomic_Time">TAI (International Atomic Time)</a>, derived entirely from atomic clocks. UTC (Universal Coordinated Time) normally ticks off each second in sync with TAI, which makes it seem deceptively easy to handle. However, UTC is defined such that when the <a href="https://en.wikipedia.org/wiki/DUT1">offset between UTC and UT1</a> gets too large, a <a href="https://en.wikipedia.org/wiki/International_Earth_Rotation_and_Reference_Systems_Service">standards bureau</a> decides that UTC needs to <a href="https://en.wikipedia.org/wiki/Leap_second#Insertion_of_leap_seconds">take a time-out</a> (known as a “leap second”) and let UT1 (the Earth’s rotation) catch up. During the leap second, UTC has a mildly nonsensical value – rather than going from 23:59:59 to 00:00:00 as usual, UTC on the leap second-affected day goes from 23:59:59 to 23:59:60 to 00:00:00. While this fulfills the goals of UTC – use constant second lengths that match up with the cesium-derived SI standard second but regularly insert leap seconds to keep UTC in sync with the Earth’s rotation – <a href="https://en.wikipedia.org/wiki/Leap_second#Examples_of_problems_associated_with_the_leap_second">leap seconds cause significant problems</a> for computers, networks, navigational systems, financial systems, air traffic control systems, and allegedly, even <a href="http://queue.acm.org/detail.cfm?id=1967009">US nuclear weapons systems</a>. Note that leap seconds aren’t even deterministic – they depend on measurements of the Earth’s rotation. Software not only chokes on the leap second, but can’t even know more than a few months in advance if there’ll even <em>be</em> a leap second.</p>
<p>The problem with leap seconds not due to shoddy coding. There are irreconcilable differences between the semantics of time that most software depend on and the semantics of UTC around leap seconds. Aside from conveniently scheduling maintenance/downtime periods to coincide with leap second days, there are two fundamental schools of thought about how to deal with leap seconds – the first is to make all software (all application software, libraries, operating systems, NTP software) aware of leap seconds and correctly operate around them. This is a nontrivial task – every calculation of times and dates and time intervals is affected, as is storage and transmission of time/date values. This is a deeply fragile and devastatingly difficult solution – if you forget to take into account leap seconds once, everything can get messed up in your software. This is not an approach to dealing with complexity that has a good record in software engineering (ask a C programmer about memory safety).</p>
<h1 id="bad-standards-utc-and-a-proven-alternative-tai-the-timezone-database">Bad standards (UTC) and a proven alternative (TAI + the timezone database)</h1>
<p>Using UTC with leap seconds for computers does not seem to be a good idea. The traditional response might be to blame programmers for failing to be sufficiently careful and detail-oriented but it’s not a useful or correct response. A standard that gives weird and non-deterministic semantics to the thing we expect to have the simplest and <strong>most</strong> deterministic semantics (outside of relativistic effects) – time itself – is a bad standard. djb’s <a href="https://cr.yp.to/talks/2015.01.07/slides-djb-20150107-a4.pdf">view</a> on cryptographic standards is applicable: if a standard leaves easily avoidable ways for the implementers of it to mess up, <strong>it’s a bad standard</strong>. Instead of having to redesign every bit of every system that manipulates/stores/transmits/computes with time to account for a nondeterministic leap second (that almost certainly doesn’t even have a good representation in whatever time formats are in use), we would like to get rid of UTC and use something like TAI (or anything that has a constant offset from TAI) to get rid of all leap second issues in one fell swoop. We can’t completely relegate UTC and leap seconds to the dustbin of history, because there <strong>are</strong> applications that need to use time that lines up nicely with days and nights: civil time, for example, will remain defined by UTC. This isn’t a problem though – as we already have a time-tested mechanism to convert time from a universal standard to a bunch of separate time standards that accounts for non-deterministic updates in time conversion rules that leap seconds would require: it’s called the <a href="https://en.wikipedia.org/wiki/Tz_database">timezone database</a>. Rather than injecting leap seconds directly into an otherwise unsullied atomic timescale, using TAI for time transfer (NTP) and internal software use and only adding leap seconds when needed with the timezone mechanism is a much cleaner separation of concerns. This is no untested proposal: this is what the GPS system (which depends on precise timing) uses. GPS time is TAI (with an offset of 19 seconds – but the offset never changes, so it’s irrelevant) and GPS navigational messages contains information to relate GPS time to UTC (how many leap seconds have been added to UTC so far).</p>
]]></description>
    <pubDate>Fri, 15 Jan 2016 00:00:00 UT</pubDate>
    <guid>https://matildah.github.io/posts/2016-01-15-leapseconds.html</guid>
    <dc:creator>Kia</dc:creator>
</item>
<item>
    <title>Introduction to MirageOS and NTP</title>
    <link>https://matildah.github.io/posts/2015-12-20-introduction.html</link>
    <description><![CDATA[<div class="info">
    Posted on December 20, 2015
    
</div>

<p>I’ve been getting started on my <a href="https://wiki.gnome.org/Outreachy/2015/DecemberMarch">Outreachy internship</a>, working with the <a href="https://mirage.io/">MirageOS</a> unikernel, (part of Xen). A unikernel is a modern way to organize application code along with systems code. Rather than having an application (along with the runtime system of whatever language it is written in) running on a fully general operating system in a hypervisor, unikernels work to allow collapse of non-necessary abstraction layers. Rather than a fully general OS that can run any application and handle its system calls, we have a set of libraries that provide similar services that an OS would provide with syscalls, along with a runtime system (for whatever language the unikernel is written in), and configuration tools that allow for each application to be compiled with the OS libraries and the runtime into a single bespoke image that can be loaded into the hypervisor and run.</p>
<p>There are several significant advantages to unikernels. All the code is executed in the same address space and context; there are no syscalls involved and no application/kernel privilege level changes (there are of course hypercalls and unikernel/hypervisor context switches but those are inevitable here). This makes analysis of the code much easier – everything besides the hypervisor itself is code that the compiler can see at one time and can optimize / typecheck appropriately. Avoiding userspace/kernel context switches is also kinder on the caches/TLBs – syscalls to the OS are replaced with function calls in the same address space! Also, while there’s no fundamental requirement for this, it’s possible to write almost all the code for a unikernel’s OS libraries in a strongly-typed functional programming language such as Haskell or OCaml as opposed to writing it in C.</p>
<p>There are a handful of servers for common network services/protocols already implemented in OCaml and that can run on MirageOS such as an <a href="https://github.com/mirage/mirage-www">http server</a> (that the MirageOS website is hosted on, a <a href="https://github.com/mirage/ocaml-dns">DNS server</a> and a <a href="https://mirage.io/blog/introducing-charrua-dhcp">DHCP server</a>, however a conspicuously missing one is NTP – the <a href="https://en.wikipedia.org/wiki/Network_Time_Protocol">Network Time Protocol</a>.</p>
<p>NTP allows a computer with a network connection to get its internal clock synchronized over the internet to a reference clock that has specialized timing hardware such as a GPS receiver or even a <a href="https://en.wikipedia.org/wiki/Atomic_clock">cesium or rubidium frequency standard</a>. An NTP server doesn’t do much – all it does is look at its time reference (either specialized timekeeping hardware or just another NTP server that itself has specialized time hardware) and answer client requests with the current time. An NTP client has to do two primary things with the information it gets from replies from an NTP server – the first is correcting the time offset between its clock so the time is, well, accurate. This isn’t enough, alas, as the timekeeping crystal oscillator in most computers isn’t too good (it doesn’t need to be, because of NTP &gt;_&lt;) – not only is its frequency inaccurate (it might run too fast, like a cheap digital watch that gains a few seconds per day), but the inaccuracy in frequency itself <strong>drifts</strong> as a function of temperature (and other factors).</p>
<p>An NTP client that wishes to try its best at keeping accurate time (as opposed to the kind that just periodically set the local clock to the time received by a reference server and completely <em>ignores</em> what happens to the local clock) needs a feedback loop that adjusts the phase (to minimize the time offset) and frequency (to minimize the frequency error) of the local clock, fed with measurements made (over the network) of reference servers. After some time, the feedback loop will converge and accurately track the local clock’s frequency offset and thus will keep accurate time without having to step the clock.</p>
<p>For this project, I’m working on implementing an NTP server and an NTP client in OCaml for the MirageOS unikernel. My plan is to make a purely functional NTP library that handles parsing/generating NTP packets and the state machine / filtering / clock discipline algorithms that is free of dependencies to UNIX. Once I am done with that, I will try to interface it to MirageOS’s clock interface (and modify it if necessary).</p>
]]></description>
    <pubDate>Sun, 20 Dec 2015 00:00:00 UT</pubDate>
    <guid>https://matildah.github.io/posts/2015-12-20-introduction.html</guid>
    <dc:creator>Kia</dc:creator>
</item>

    </channel>
</rss>
